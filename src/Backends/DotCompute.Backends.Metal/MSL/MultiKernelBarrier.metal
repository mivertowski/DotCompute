// ===========================================================================
// Multi-Kernel Barrier - Metal Shading Language Implementation
// Copyright (c) 2025 Michael Ivertowski
// Licensed under the MIT License
// ===========================================================================
//
// Provides GPU-side barrier synchronization for persistent Ring Kernels.
// Uses Metal's atomic operations for cross-kernel coordination.
//
// Memory Layout (16 bytes, matches C# struct):
//   - ParticipantCount: int (4 bytes)
//   - ArrivedCount: atomic<int> (4 bytes)
//   - Generation: atomic<int> (4 bytes)
//   - Flags: atomic<int> (4 bytes)
//
// Performance (Apple Silicon M2+):
//   - Typical latency: 5-50Î¼s for 2-8 kernels
//   - Unified memory enables zero-copy CPU/GPU synchronization
// ===========================================================================

#include <metal_stdlib>
#include <metal_atomic>
using namespace metal;

// Barrier state flags (matches C# constants)
constant int BARRIER_FLAG_ACTIVE = 0x0001;
constant int BARRIER_FLAG_TIMEOUT = 0x0002;
constant int BARRIER_FLAG_FAILED = 0x0004;

/// <summary>
/// Multi-kernel barrier structure (matches C# MetalMultiKernelBarrier layout).
/// </summary>
struct MultiKernelBarrierState {
    int participant_count;           // Number of kernels that must arrive
    atomic_int arrived_count;        // Atomic counter for arrived kernels
    atomic_int generation;           // Generation number for barrier reuse
    atomic_int flags;                // State flags (active, timeout, failed)
};

// ===========================================================================
// Kernel 1: Wait at Multi-Kernel Barrier
// ===========================================================================

/// <summary>
/// Waits at a multi-kernel barrier until all participants arrive.
/// </summary>
/// <param name="barrier">Barrier state buffer (unified memory).</param>
/// <param name="thread_id">Thread position in grid (only thread 0 participates).</param>
/// <remarks>
/// This kernel implements a generation-based barrier to prevent ABA problems:
/// 1. Thread 0 atomically increments arrived_count
/// 2. Spin-waits until arrived_count == participant_count
/// 3. Last thread to arrive increments generation and resets arrived_count
/// 4. All threads observe generation change and exit
///
/// Uses sequential consistency for cross-kernel visibility.
/// </remarks>
kernel void wait_at_multi_kernel_barrier_kernel(
    device MultiKernelBarrierState* barrier [[buffer(0)]],
    uint thread_id [[thread_position_in_grid]])
{
    // Only thread 0 of each kernel participates in barrier
    if (thread_id != 0) {
        return;
    }

    // Check if barrier is active
    int current_flags = atomic_load_explicit(&barrier->flags, memory_order_acquire);
    if ((current_flags & BARRIER_FLAG_ACTIVE) == 0) {
        return; // Barrier not active
    }

    // Check for failure state
    if (current_flags & BARRIER_FLAG_FAILED) {
        return; // Barrier failed, abort
    }

    // Capture current generation before arriving
    int current_generation = atomic_load_explicit(&barrier->generation, memory_order_acquire);

    // Atomically increment arrived count
    int arrived = atomic_fetch_add_explicit(&barrier->arrived_count, 1, memory_order_acq_rel);

    // Check if this thread is the last to arrive
    if (arrived + 1 == barrier->participant_count) {
        // Last thread arrived - advance generation and reset counter
        atomic_store_explicit(&barrier->arrived_count, 0, memory_order_release);
        atomic_fetch_add_explicit(&barrier->generation, 1, memory_order_release);
    } else {
        // Not last thread - spin-wait for generation to change
        // This indicates all participants have arrived
        int max_spins = 100000000; // ~10 second timeout on Apple Silicon
        int spin_count = 0;

        while (spin_count < max_spins) {
            int observed_generation = atomic_load_explicit(&barrier->generation, memory_order_acquire);

            // Check if generation changed (barrier completed)
            if (observed_generation != current_generation) {
                break; // Barrier completed
            }

            // Check for failure
            int observed_flags = atomic_load_explicit(&barrier->flags, memory_order_acquire);
            if (observed_flags & BARRIER_FLAG_FAILED) {
                break; // Barrier failed
            }

            spin_count++;

            // Brief pause to reduce memory contention
            if ((spin_count & 0xFFFF) == 0) {
                threadgroup_barrier(mem_flags::mem_none);
            }
        }

        // If we timed out, mark barrier as timed out
        if (spin_count >= max_spins) {
            atomic_fetch_or_explicit(&barrier->flags, BARRIER_FLAG_TIMEOUT, memory_order_release);
        }
    }
}

// ===========================================================================
// Kernel 2: Reset Multi-Kernel Barrier
// ===========================================================================

/// <summary>
/// Resets a multi-kernel barrier to its initial state.
/// </summary>
/// <param name="barrier">Barrier state buffer (unified memory).</param>
/// <param name="thread_id">Thread position in grid (only thread 0 executes).</param>
/// <remarks>
/// Resets barrier for reuse while maintaining generation number.
/// Only thread 0 performs the reset to ensure atomicity.
/// </remarks>
kernel void reset_multi_kernel_barrier_kernel(
    device MultiKernelBarrierState* barrier [[buffer(0)]],
    uint thread_id [[thread_position_in_grid]])
{
    // Only thread 0 resets the barrier
    if (thread_id != 0) {
        return;
    }

    // Reset arrived count to 0
    atomic_store_explicit(&barrier->arrived_count, 0, memory_order_release);

    // Clear timeout and failed flags, keep active flag
    int current_flags = atomic_load_explicit(&barrier->flags, memory_order_acquire);
    int new_flags = (current_flags & BARRIER_FLAG_ACTIVE);
    atomic_store_explicit(&barrier->flags, new_flags, memory_order_release);

    // Note: We do NOT reset generation - it increments monotonically
    // to prevent ABA problems with barrier reuse
}

// ===========================================================================
// Kernel 3: Mark Barrier as Failed
// ===========================================================================

/// <summary>
/// Marks a multi-kernel barrier as failed (for error recovery).
/// </summary>
/// <param name="barrier">Barrier state buffer (unified memory).</param>
/// <param name="thread_id">Thread position in grid (only thread 0 executes).</param>
/// <remarks>
/// Sets the FAILED flag to signal all waiting threads to abort.
/// Used when a participant kernel crashes or encounters an error.
/// </remarks>
kernel void mark_barrier_failed_kernel(
    device MultiKernelBarrierState* barrier [[buffer(0)]],
    uint thread_id [[thread_position_in_grid]])
{
    // Only thread 0 marks failure
    if (thread_id != 0) {
        return;
    }

    // Set failed flag using atomic OR
    atomic_fetch_or_explicit(&barrier->flags, BARRIER_FLAG_FAILED, memory_order_release);

    // Optionally clear active flag
    int current_flags = atomic_load_explicit(&barrier->flags, memory_order_acquire);
    int new_flags = current_flags & ~BARRIER_FLAG_ACTIVE;
    atomic_store_explicit(&barrier->flags, new_flags, memory_order_release);
}
