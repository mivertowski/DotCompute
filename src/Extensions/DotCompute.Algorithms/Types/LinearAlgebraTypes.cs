// Copyright (c) 2025 Michael Ivertowski
// Licensed under the MIT License. See LICENSE file in the project root for license information.

namespace DotCompute.Algorithms;

/// <summary>
/// Kernel execution parameters for GPU linear algebra operations.
/// </summary>
/// <remarks>
/// <para>
/// Configures work group dimensions, memory usage, and tiling strategy
/// for GPU kernel execution. Parameters are optimized per-operation based
/// on matrix size, hardware capabilities, and memory constraints.
/// </para>
/// <para>
/// Generated by adaptive kernel selection based on runtime profiling
/// and hardware characteristics.
/// </para>
/// </remarks>
internal class KernelExecutionParameters
{
    /// <summary>
    /// Gets or sets the global work size (total number of work items).
    /// </summary>
    /// <remarks>
    /// <para>
    /// Defines total computational workload across all dimensions.
    /// For matrix operations, typically [M, N] or [M, N, 1].
    /// </para>
    /// <para><b>Example</b>: [1024, 1024] for 1024×1024 matrix operation</para>
    /// </remarks>
    public IReadOnlyList<int> GlobalWorkSize { get; set; } = Array.Empty<int>();

    /// <summary>
    /// Gets or sets the local work size (work group dimensions).
    /// </summary>
    /// <remarks>
    /// <para>
    /// Defines work group size for shared memory and synchronization.
    /// Must divide GlobalWorkSize evenly. Common values: [16, 16], [32, 8], [64, 4].
    /// </para>
    /// <para><b>Constraint</b>: Product must not exceed MaxWorkGroupSize</para>
    /// <para><b>Optimization</b>: Choose multiples of warp/wavefront size (32/64)</para>
    /// </remarks>
    public IReadOnlyList<int> LocalWorkSize { get; set; } = Array.Empty<int>();

    /// <summary>
    /// Gets or sets the shared memory size in bytes.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Amount of fast on-chip shared memory (local memory) allocated per work group.
    /// Critical for tiled matrix multiply and reduction operations.
    /// </para>
    /// <para><b>CUDA</b>: Shared memory per SM (48-164 KB depending on architecture)</para>
    /// <para><b>OpenCL</b>: Local memory size varies by device (16-64 KB typical)</para>
    /// <para><b>Metal</b>: Threadgroup memory (32-64 KB on Apple Silicon)</para>
    /// </remarks>
    public int SharedMemorySize { get; set; }

    /// <summary>
    /// Gets or sets whether to use shared memory optimization.
    /// </summary>
    /// <remarks>
    /// <para>
    /// When true, enables tiled algorithms that exploit shared memory
    /// for data reuse. Provides 10-100x speedup for suitable operations
    /// but requires careful memory management.
    /// </para>
    /// <para><b>Trade-off</b>: Increased occupancy vs. shared memory per work group</para>
    /// </remarks>
    public bool UseSharedMemory { get; set; }

    /// <summary>
    /// Gets or sets the tile size for tiled algorithms.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Dimension of square tiles for tiled matrix operations. Balances
    /// data reuse against shared memory capacity and register pressure.
    /// </para>
    /// <para><b>Default</b>: 16 (good balance for most hardware)</para>
    /// <para><b>Typical Range</b>: 8-32 depending on shared memory availability</para>
    /// <para><b>Constraint</b>: Tile size² × element size ≤ SharedMemorySize</para>
    /// </remarks>
    public int TileSize { get; set; } = 16;
}

/// <summary>
/// Matrix properties for adaptive algorithm selection.
/// </summary>
/// <remarks>
/// <para>
/// Analyzes matrix characteristics to select optimal algorithms and
/// kernel configurations. Properties include size, structure, and
/// numerical conditioning information.
/// </para>
/// <para>
/// Used by adaptive kernel selection to choose between algorithms
/// (e.g., sparse vs. dense, iterative vs. direct methods).
/// </para>
/// </remarks>
public class MatrixProperties
{
    /// <summary>
    /// Gets or sets the number of rows in the matrix.
    /// </summary>
    /// <remarks>
    /// Number of rows (M dimension for M×N matrix).
    /// </remarks>
    public int Rows { get; set; }

    /// <summary>
    /// Gets or sets the number of columns in the matrix.
    /// </summary>
    /// <remarks>
    /// Number of columns (N dimension for M×N matrix).
    /// </remarks>
    public int Columns { get; set; }

    /// <summary>
    /// Gets or sets the total matrix size in elements.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Total number of elements (M × N for M×N matrix). Used to determine
    /// memory requirements and select between in-core and out-of-core algorithms.
    /// </para>
    /// <para><b>Decision Threshold</b>: &gt; 100M elements may require out-of-core processing</para>
    /// </remarks>
    public long Size { get; set; }

    /// <summary>
    /// Gets or sets the sparsity ratio (fraction of zero elements).
    /// </summary>
    /// <remarks>
    /// <para>
    /// Ratio of zero elements to total elements (0.0 = dense, 1.0 = all zeros).
    /// Sparse matrices (&gt; 0.9) benefit from specialized sparse algorithms.
    /// </para>
    /// <para><b>Sparse Threshold</b>: &gt; 0.9 typically justifies sparse storage/algorithms</para>
    /// <para><b>Storage Savings</b>: CSR/CSC format reduces memory by (1 - sparsity) fraction</para>
    /// </remarks>
    public float SparsityRatio { get; set; }

    /// <summary>
    /// Gets or sets the condition number of the matrix.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Condition number κ(A) = ||A|| × ||A^(-1)|| measures numerical sensitivity.
    /// Large values indicate ill-conditioned matrices prone to rounding errors.
    /// </para>
    /// <para><b>Well-conditioned</b>: κ &lt; 100</para>
    /// <para><b>Moderately conditioned</b>: 100 &lt; κ &lt; 10^6</para>
    /// <para><b>Ill-conditioned</b>: κ &gt; 10^6 (requires high-precision algorithms)</para>
    /// <para><b>Action</b>: High κ triggers iterative refinement or mixed-precision methods</para>
    /// </remarks>
    public float ConditionNumber { get; set; }

    /// <summary>
    /// Gets or sets whether the matrix is symmetric.
    /// </summary>
    /// <remarks>
    /// <para>
    /// True if A = A^T (symmetric). Symmetric matrices enable specialized
    /// algorithms (Cholesky, symmetric eigensolvers) with 2x speedup.
    /// </para>
    /// <para><b>Storage</b>: Only N(N+1)/2 elements need storing (50% savings)</para>
    /// <para><b>Algorithms</b>: Enables Lanczos, symmetric QR, Cholesky decomposition</para>
    /// </remarks>
    public bool IsSymmetric { get; set; }

    /// <summary>
    /// Gets or sets whether the matrix is positive-definite.
    /// </summary>
    /// <remarks>
    /// <para>
    /// True if symmetric and x^T A x &gt; 0 for all x ≠ 0. Positive-definite
    /// matrices enable Cholesky decomposition (2x faster than LU).
    /// </para>
    /// <para><b>Requirements</b>: Must be symmetric first</para>
    /// <para><b>Benefits</b>: Cholesky is numerically stable and efficient</para>
    /// <para><b>Applications</b>: Least squares, optimization, covariance matrices</para>
    /// </remarks>
    public bool IsPositiveDefinite { get; set; }

    /// <summary>
    /// Gets or sets whether high precision computation is required.
    /// </summary>
    /// <remarks>
    /// <para>
    /// True if the matrix requires double-precision (FP64) computation due to
    /// ill-conditioning or numerical sensitivity. Automatically set when
    /// condition number exceeds threshold.
    /// </para>
    /// <para><b>Trigger</b>: Typically set when κ(A) &gt; 10^6</para>
    /// <para><b>Impact</b>: Uses FP64 operations (slower on consumer GPUs)</para>
    /// </remarks>
    public bool RequiresHighPrecision { get; set; }
}

/// <summary>
/// GPU hardware information for kernel optimization.
/// </summary>
/// <remarks>
/// <para>
/// Hardware characteristics queried from GPU device for adaptive kernel
/// configuration. Used to tune work group sizes, memory usage, and
/// algorithm selection based on device capabilities.
/// </para>
/// <para>
/// Obtained via platform-specific APIs (CUDA: cudaDeviceGetAttribute,
/// OpenCL: clGetDeviceInfo, Metal: MTLDevice properties).
/// </para>
/// </remarks>
[System.Diagnostics.CodeAnalysis.SuppressMessage("Design", "CA1034:Nested types should not be visible",
    Justification = "Type made public to fix CA0050/CA0051 accessibility warnings. Used in public method signatures.")]
public class HardwareInfo
{
    /// <summary>
    /// Gets or sets the global memory size in bytes.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Total GPU device memory (VRAM) available. Determines maximum
    /// matrix sizes and whether out-of-core algorithms are needed.
    /// </para>
    /// <para><b>Typical Values</b>:</para>
    /// <list type="bullet">
    /// <item>Consumer GPUs: 4-24 GB</item>
    /// <item>Professional GPUs: 16-80 GB</item>
    /// <item>Data Center GPUs: 40-80 GB (A100, H100)</item>
    /// </list>
    /// </remarks>
    public long GlobalMemorySize { get; set; }

    /// <summary>
    /// Gets or sets the shared memory size per work group in bytes.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Fast on-chip memory shared within a work group (CUDA: shared memory,
    /// OpenCL: local memory, Metal: threadgroup memory).
    /// </para>
    /// <para><b>NVIDIA</b>: 48-164 KB per SM depending on architecture</para>
    /// <para><b>AMD</b>: 64 KB LDS per CU</para>
    /// <para><b>Apple Silicon</b>: 32-64 KB threadgroup memory</para>
    /// </remarks>
    public int SharedMemorySize { get; set; }

    /// <summary>
    /// Gets or sets the maximum work group size (threads per block).
    /// </summary>
    /// <remarks>
    /// <para>
    /// Maximum number of threads that can execute concurrently in a
    /// single work group. Constrains LocalWorkSize configuration.
    /// </para>
    /// <para><b>Typical Values</b>: 256-1024 threads</para>
    /// <para><b>NVIDIA</b>: 1024 threads per block (most architectures)</para>
    /// <para><b>AMD</b>: 256-1024 threads per workgroup</para>
    /// </remarks>
    public int MaxWorkGroupSize { get; set; }

    /// <summary>
    /// Gets or sets the preferred work group size multiple (warp/wavefront size).
    /// </summary>
    /// <remarks>
    /// <para>
    /// SIMD width for optimal execution efficiency. Work group sizes
    /// should be multiples of this value to avoid partial warps/wavefronts.
    /// </para>
    /// <para><b>NVIDIA</b>: 32 (warp size)</para>
    /// <para><b>AMD</b>: 64 (wavefront size)</para>
    /// <para><b>Intel</b>: 8-32 depending on architecture</para>
    /// <para><b>Apple Silicon</b>: 32 (SIMD group size)</para>
    /// </remarks>
    public int PreferredWorkGroupSizeMultiple { get; set; }

    /// <summary>
    /// Gets or sets the number of compute units (SMs/CUs).
    /// </summary>
    /// <remarks>
    /// <para>
    /// Number of parallel processing units on GPU. Used to determine
    /// optimal grid size and occupancy targets.
    /// </para>
    /// <para><b>Examples</b>:</para>
    /// <list type="bullet">
    /// <item>RTX 4090: 128 SMs</item>
    /// <item>RTX 2000 Ada: 28 SMs</item>
    /// <item>A100: 108 SMs</item>
    /// <item>RX 7900 XTX: 96 CUs</item>
    /// </list>
    /// <para><b>Grid Sizing</b>: Typically aim for 2-4x compute units for occupancy</para>
    /// </remarks>
    public int ComputeUnits { get; set; }

    /// <summary>
    /// Gets or sets whether the hardware supports tensor cores.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Indicates availability of specialized hardware for mixed-precision
    /// matrix multiplication (e.g., NVIDIA Tensor Cores, AMD Matrix Cores).
    /// </para>
    /// <para><b>NVIDIA</b>: Volta, Turing, Ampere, Ada, Hopper architectures</para>
    /// <para><b>AMD</b>: RDNA 3 and CDNA architectures</para>
    /// </remarks>
    public bool SupportsTensorCores { get; set; }

    /// <summary>
    /// Gets or sets the warp/wavefront size for the hardware.
    /// </summary>
    /// <remarks>
    /// <para>
    /// SIMD execution width. Same as PreferredWorkGroupSizeMultiple but
    /// exposed with CUDA-style naming for convenience.
    /// </para>
    /// <para><b>NVIDIA</b>: 32 (warp)</para>
    /// <para><b>AMD</b>: 64 (wavefront)</para>
    /// </remarks>
    public int? WarpSize { get; set; }

    /// <summary>
    /// Gets or sets whether the hardware supports double precision (FP64).
    /// </summary>
    /// <remarks>
    /// <para>
    /// Indicates native hardware support for 64-bit floating point operations.
    /// Consumer GPUs often have reduced FP64 performance (1/32 of FP32).
    /// </para>
    /// <para><b>Full FP64</b>: Professional/datacenter GPUs (Tesla, Instinct)</para>
    /// <para><b>Reduced FP64</b>: Consumer GPUs (GeForce, Radeon)</para>
    /// </remarks>
    public bool SupportsDoublePrecision { get; set; }
}

/// <summary>
/// Adaptive kernel configuration based on runtime analysis.
/// </summary>
/// <remarks>
/// <para>
/// Dynamic configuration determined by analyzing matrix properties,
/// hardware capabilities, and numerical requirements. Selects optimal
/// algorithms and precision levels at runtime.
/// </para>
/// <para>
/// Generated by adaptive kernel selection system using machine learning
/// or heuristics based on historical performance data.
/// </para>
/// </remarks>
internal class AdaptiveKernelConfig
{
    /// <summary>
    /// Gets or sets whether to use sparse matrix optimizations.
    /// </summary>
    /// <remarks>
    /// <para>
    /// When true, uses CSR/CSC sparse storage formats and sparse
    /// algorithms. Enabled when sparsity exceeds threshold (typically &gt; 0.9).
    /// </para>
    /// <para><b>Performance</b>: 10-100x speedup for highly sparse matrices</para>
    /// <para><b>Overhead</b>: Sparse overhead not beneficial below threshold</para>
    /// </remarks>
    public bool UseSparseOptimizations { get; set; }

    /// <summary>
    /// Gets or sets the sparsity threshold for sparse algorithm selection.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Minimum sparsity ratio (0.0-1.0) required to enable sparse algorithms.
    /// Balances sparse algorithm benefits against format conversion overhead.
    /// </para>
    /// <para><b>Typical Range</b>: 0.85-0.95</para>
    /// <para><b>Default</b>: 0.9 (90% zeros)</para>
    /// </remarks>
    public float SparsityThreshold { get; set; }

    /// <summary>
    /// Gets or sets whether to use out-of-core algorithms.
    /// </summary>
    /// <remarks>
    /// <para>
    /// When true, uses algorithms that process matrices in blocks/tiles
    /// larger than GPU memory. Required for very large matrices.
    /// </para>
    /// <para><b>Trigger</b>: Matrix size exceeds 80% of GlobalMemorySize</para>
    /// <para><b>Trade-off</b>: Higher memory throughput vs. increased complexity</para>
    /// </remarks>
    public bool UseOutOfCoreAlgorithm { get; set; }

    /// <summary>
    /// Gets or sets the block size for blocked algorithms.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Dimension of matrix blocks for cache-oblivious algorithms and
    /// out-of-core processing. Balances cache utilization and overhead.
    /// </para>
    /// <para><b>Default</b>: 64 (good L1 cache fit for most CPUs)</para>
    /// <para><b>Typical Range</b>: 32-128 depending on cache hierarchy</para>
    /// </remarks>
    public int BlockSize { get; set; } = 64;

    /// <summary>
    /// Gets or sets whether high numerical precision is required.
    /// </summary>
    /// <remarks>
    /// <para>
    /// True for ill-conditioned matrices (high condition number) requiring
    /// double precision or iterative refinement. False allows float32 for speed.
    /// </para>
    /// <para><b>Trigger</b>: Condition number &gt; 10^6</para>
    /// <para><b>Impact</b>: 2x memory, 2-4x slower (GPU architecture dependent)</para>
    /// </remarks>
    public bool RequiresHighPrecision { get; set; }

    /// <summary>
    /// Gets or sets whether to use mixed-precision arithmetic.
    /// </summary>
    /// <remarks>
    /// <para>
    /// When true, uses float32 for computation with float64 accumulation.
    /// Provides accuracy close to double precision with near-single precision speed.
    /// </para>
    /// <para><b>Applications</b>: Moderately ill-conditioned problems</para>
    /// <para><b>Performance</b>: 1.5-2x faster than pure double precision</para>
    /// <para><b>Hardware</b>: Requires tensor core support for optimal benefit</para>
    /// </remarks>
    public bool UseMixedPrecision { get; set; }

    /// <summary>
    /// Gets or sets the optimal work group size for this configuration.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Work group size that maximizes occupancy and performance for
    /// current matrix and hardware. Determined by empirical tuning or
    /// analytical model.
    /// </para>
    /// <para><b>Default</b>: 256 (good balance for most operations)</para>
    /// <para><b>Constraint</b>: Must be multiple of PreferredWorkGroupSizeMultiple</para>
    /// <para><b>Range</b>: 64-1024 depending on shared memory usage</para>
    /// </remarks>
    public int OptimalWorkGroupSize { get; set; } = 256;
}
